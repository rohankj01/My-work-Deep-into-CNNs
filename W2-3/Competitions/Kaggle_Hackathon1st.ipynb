{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle Hackathon",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u1Y4y06PPYajAqtaB7TjlwC5k1EhmRqq",
      "authorship_tag": "ABX9TyMJadceG1yeazf6NS4V1PZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohankj01/My-work-Deep-into-CNNs/blob/main/W2-3/Competitions/Kaggle_Hackathon1st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxIl94GpnceE"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CTIL14Zny5s"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rylXct4n5Bc"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHbMjUG4orS4"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_57bc2Zo7iK"
      },
      "source": [
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc2-6H5PprK0"
      },
      "source": [
        "!kaggle competitions download -c tabular-playground-series-jun-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arGZC4m8pzmR"
      },
      "source": [
        "! mkdir train\n",
        "! unzip train.csv.zip -d train\n",
        "! mkdir test\n",
        "! unzip test.csv.zip -d test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZrdZJp5qTL9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmS2EYpXqiaT"
      },
      "source": [
        "train = pd.read_csv('train/train.csv')\n",
        "test = pd.read_csv('test/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUBdYGssqrGs"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQufxy2uqsjN"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Qco-c7713w"
      },
      "source": [
        "train = train.drop(['Name','Ticket','PassengerId'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4RRhHA7BUmO"
      },
      "source": [
        "one_hot_train1 = pd.concat([train, pd.get_dummies(train[['Embarked','Sex']])], axis=1)\n",
        "one_hot_train2 = pd.concat([one_hot_train1, pd.get_dummies(train['Pclass'])], axis=1)\n",
        "one_hot_train2['FamNo'] = one_hot_train2['SibSp'] + one_hot_train2['Parch']\n",
        "one_hot_train2['Age'] = one_hot_train2['Age'].replace(np.nan, 39.00)\n",
        "one_hot_train2['Age'] = one_hot_train2['Age']/87.00\n",
        "\n",
        "one_hot_train2['Fare'] = one_hot_train2['Fare']/(one_hot_train2['Fare'].max())\n",
        "one_hot_train2['FamNo'] = one_hot_train2['FamNo']/(one_hot_train2['FamNo'].max())\n",
        "processed_data = one_hot_train2.drop(['Sex','Pclass','Embarked','Embarked_C','Sex_female','SibSp','Cabin','Parch',1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwJnmhdpEEQ2"
      },
      "source": [
        "processed_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "392BWOZWgtIO"
      },
      "source": [
        "test = test.drop(['Name','Ticket','PassengerId'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrmZW8lxgxWo"
      },
      "source": [
        "one_hot_test1 = pd.concat([test, pd.get_dummies(test[['Embarked','Sex']])], axis=1)\n",
        "one_hot_test2 = pd.concat([one_hot_test1, pd.get_dummies(test['Pclass'])], axis=1)\n",
        "one_hot_test2['FamNo'] = one_hot_test2['SibSp'] + one_hot_test2['Parch']\n",
        "one_hot_test2['Age'] = one_hot_test2['Age'].replace(np.nan, 39.00)\n",
        "one_hot_test2['Age'] = one_hot_test2['Age']/87.00\n",
        "\n",
        "one_hot_test2['Fare'] = one_hot_test2['Fare']/(one_hot_test2['Fare'].max())\n",
        "one_hot_test2['FamNo'] = one_hot_test2['FamNo']/(one_hot_test2['FamNo'].max())\n",
        "processed_data_test = one_hot_test2.drop(['Sex','Pclass','Embarked','Embarked_C','Sex_female','SibSp','Cabin','Parch',1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY-kUZGYm8HS"
      },
      "source": [
        "processed_data_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeFVn-3536Jw"
      },
      "source": [
        "features, targets = processed_data.drop(['Survived'], axis=1), processed_data['Survived']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyjg3coQ4rvR"
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkkWPjpvbCyg"
      },
      "source": [
        "def error_term_formula(x, y, output):\n",
        "    error_term = (y-output)*sigmoid_prime(x)*x\n",
        "    return error_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6FLhktMbJLx"
      },
      "source": [
        "# Neural Network hyperparameters\n",
        "epochs = 100\n",
        "learnrate = 0.001\n",
        "\n",
        "# Training function\n",
        "def train_nn(features, targets, epochs, learnrate):\n",
        "    \n",
        "    # Use to same seed to make debugging easier\n",
        "    np.random.seed(42)\n",
        "\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features.values, targets):\n",
        "            # Loop through all records, x is the input, y is the target\n",
        "\n",
        "            # Activation of the output unit\n",
        "            #   Notice we multiply the inputs and the weights here \n",
        "            #   rather than storing h as a separate variable \n",
        "            output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "            # The error term\n",
        "            error_term = error_term_formula(x, y, output)\n",
        "\n",
        "            # The gradient descent step, the error times the gradient times the inputs\n",
        "            del_w += error_term\n",
        "\n",
        "        # Update the weights here. The learning rate times the \n",
        "        # change in weights\n",
        "        # don't have to divide by n_records since it is compensated by the learning rate\n",
        "        weights += learnrate * del_w #/ n_records  \n",
        "\n",
        "        # Printing out the mean square error on the training set\n",
        "        if e % (epochs / 10) == 0:\n",
        "            out = sigmoid(np.dot(features, weights))\n",
        "            loss = np.mean(error_formula(targets, out))\n",
        "            print(\"Epoch:\", e)\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            print(\"=========\")\n",
        "    print(\"Finished training!\")\n",
        "    return weights\n",
        "    \n",
        "weights = train_nn(features, targets, epochs, learnrate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}